{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through https://www.datacamp.com/blog/yolo-object-detection-explained\n",
    "### Write in points what you understood. You can write in many points as you like.\n",
    "### You can also refer other blogs related to YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction </h1>\n",
    "<ul>\n",
    "<li>YOLO is a real-time object detection algorithm, object detection in computer vision involves identifying and localizing objects within images or videos using bounding boxes.</li>\n",
    "<li>Fast R-CNN cannot be used in real-time, because it takes 2-3 seconds, but YOLO, only one forward pass is required through the network to make the final prediction. </li>\n",
    "<li>yolo reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities.</li>\n",
    "\n",
    "<li>YOLO is a fast convolutional network that predicts multiple bounding boxes and class probabilities for images. It trains on full images and optimizes detection performance, reducing latency to less than 25 milliseconds. </li>\n",
    "\n",
    "<li>YOLO, trained on the ImageNet-1000 dataset, achieves 88% accuracy on ImageNet 2012 validation, comparable to GoogLeNet. It uses fewer layers and filters, and a sum-squared error loss function for easy optimization.</li>\n",
    "\n",
    "<li>The architecture divides an image into SxS grids, detecting objects whose center is in that grid. Each grid predicts bounding boxes with confidence scores, indicating object presence and how precise it predicts the bounding box coordinates.</li>\n",
    "\n",
    "<li>YOLO is a fast, efficient real-time system capable of processing images at 45 FPS and exceeding the mean Average Precision (mAP) of other systems.</li>\n",
    "\n",
    "<li>.YOLO has very few background mistakes and a significantly higher accuracy than other state-of-the-art models.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Architercture</h1>\n",
    "<ul>\n",
    "<li> YOLO architecture is similar to GoogleNet.It has overall 24 convolutional layers, four max-pooling layers, and two fully connected layers. </li>\n",
    "<li> input image is resized into 448x448 before going through convolution network</li>\n",
    "<li>to reduce the number of channels 1x1 convolution is applied, which is then followed by 3x3 convolution to generate cuboidal output</li>\n",
    "<li>ReLU activation function is used, except for the last layer, which use linear activation function</li>\n",
    "<li>batch normalization and dropout are done to regularize the model and prevent overfitting</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>How YOLO object detection works</h1>\n",
    "<ul>\n",
    "<li> Residual blocks: This step involves dividing the original image into NxN grid cells, each predicting the object's class and probability/confidence value.</li>\n",
    "<li>Bounding box regression: YOLO determines bounding boxes for images, using a single regression module. It uses Y = [pc, bx, by, bh, bw, c1, c2] for each bounding box, including probability score, coordinates, and classes(which can be as many as required). </li>\n",
    "<li>Intersection Over Unions(IOU): YOLO uses an IOU threshold to discard irrelevant grid box candidates, focusing on those with an IOU > threshold, ignoring those with an IOU â‰¤ threshold.</li>\n",
    "<li>Non-Max Suppression(NMS):NMS can be used to select boxes with the highest probability score of detection, as setting a threshold for IOU may not be sufficient.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evolution of YOLO</h1>\n",
    "<ol>\n",
    "<li>\n",
    "YOLO or YOLOv1: \n",
    "<ul> \n",
    "<li>Single-shot object recognition was first introduced by the original YOLO, which divided the image into a grid and predicted bounding boxes and class probabilities in a unified way.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv2/YOLO9000:\n",
    "<ul>\n",
    "<li>Added Darknet-19 architecture, which uses batch normalization, and improved speed and accuracy by using anchor boxes for better localization.</li>\n",
    "<li>Presented the idea of multi-scale detection with feature maps from several levels.</li>\n",
    "<li> Increased the number of object classes to over 9,000 categories by combining WordNet's hierarchical categorization algorithm with YOLO.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv3:\n",
    "<ul>\n",
    "<li> More feature pyramid scales were added to improve accuracy and allow for numerous resolutions of detection.</li>\n",
    "<li>Independent logistic classifiers have been used to accurately predict class of bounding boxes instead of using softmax as in YOLOv2.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv4:\n",
    "<ul>\n",
    "<li> By using CSPDarknet53 as the backbone and leveraging Cross-Stage Partial connections to boost information flow, accuracy and speed were further increased.</li>\n",
    "<li>Used a variety of methods, including spatial pyramid pooling, PANet, and Mish activation.</li>\n",
    "<li>performs hyper-paramter selection using genetic algorithms</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOR:\n",
    "<ul>\n",
    "<li>It is based on the unified network which is a combination of explicit(conscious) and implicit(subconscious) knowledge approaches.</li>\n",
    "<li>More robost architecture is created based on, feature alignment, prediction alignment for object detection, and canonical representation for multi-task learning.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOX:\n",
    "<ul>\n",
    "<li>Introduced a flexible and scalable anchor-free detector that makes use of a detection head known as Decoupled Head.</li>\n",
    "<li>Emphasized efficiency and adaptability for a range of deployment scenarios.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv5:\n",
    "<ul>\n",
    "<li>First YOLO version implemented in Pytorch, uses CSPDarknet53 as its backbone. </li>\n",
    "<li>Includes a Focus layer, reducing layers and parameters while increasing forward and backward speed without significantly impacting the mAP.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv6:\n",
    "<ul>\n",
    "<li>Meituan, a Chinese e-commerce company, has released the YOLOv6 framework, designed for industrial applications. </li>\n",
    "<li>the backbone was inspired by the original one-stage YOLO architecture</li>\n",
    "<li>significant improvements to previous YOLOv5 are: hardware-friendly backbone, efficient decoupled head, and a more effective training strategy. </li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv7:\n",
    "<ul>\n",
    "<li>It surpassed all the previous models in terms of accuracy and speed.</li>\n",
    "<li>YOLOv7 has improved its architecture by integrating E-ELAN and scaling it with models like YOLOv4, Scaled YOLOv4, and YOLO-R, enhancing learning diversity and inference speed.</li>\n",
    "<li>YOLOv7's trainable bag-of-freebies approach enhances model accuracy without increasing training costs, enhancing both inference speed and detection accuracy.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "YOLOv8:\n",
    "<ul>\n",
    "<li>By leveraging efficiant neural network architecture, YOLOv8 can process images in real-time, enabling quick and reliable object recognition.</li>\n",
    "<li>YOLOv8 excels in detecting small objects, enhancing it appliability in senarios where precision detection is crucial.</li>\n",
    "<li>Anchor-free architecture, it also employs a multi-scale prediction method, allowing it to detect objects at various scales within an images.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Additional Refrences</h3>\n",
    "<pre>\n",
    "https://www.geeksforgeeks.org/yolo-you-only-look-once-real-time-object-detection/\n",
    "https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006\n",
    "https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65\n",
    "https://keylabs.ai/blog/comparing-yolov8-and-yolov7-whats-new/\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
